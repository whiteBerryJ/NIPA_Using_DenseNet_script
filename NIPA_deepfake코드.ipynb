{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "# data_path = '/home/workspace/data/.train/.task146/data/train'\n",
    "# os.chdir(data_path)\n",
    "# os.getcwd()\n",
    "# os.listdir()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from PIL import ImageFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/workspace/data/.train/.task146/data/train'\n",
    "os.chdir(data_path)\n",
    "os.getcwd()\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# (real:0, fake:1)\n",
    "df = pd.read_csv('open_train_label.txt', sep=' ', header=None)\n",
    "df.columns = ['filename','label']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as Image\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(20):\n",
    "    plt.subplot(4,5,i+1)\n",
    "    img = Image.imread(df.filename[i+3000])\n",
    "    plt.title(df.label[i])\n",
    "    plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label = df.label.astype(str)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "image_size = (331,331)\n",
    "batch_size = 32\n",
    "directory = data_path\n",
    "train_datagen = ImageDataGenerator(rescale=1./255., validation_split=0.2)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "                                    df,\n",
    "                                    directory,\n",
    "                                    x_col = 'filename',\n",
    "                                    y_col = 'label',\n",
    "                                    subset = 'training',\n",
    "                                    target_size = image_size,\n",
    "                                    color_mode = 'rgb',\n",
    "                                    class_mode = 'binary',\n",
    "                                    batch_size = batch_size,\n",
    "                                    shuffle = True,\n",
    "                                    seed = 33)\n",
    "valid_generator = train_datagen.flow_from_dataframe(\n",
    "                                    df,\n",
    "                                    directory,\n",
    "                                    x_col = 'filename',\n",
    "                                    y_col = 'label',\n",
    "                                    subset = 'validation',\n",
    "                                    target_size = image_size,\n",
    "                                    color_mode = 'rgb',\n",
    "                                    class_mode = 'binary',\n",
    "                                    batch_size = batch_size,\n",
    "                                    shuffle = True,\n",
    "                                    seed = 33)\n",
    "\n",
    "\n",
    "\n",
    "print(train_generator.n)\n",
    "print(valid_generator.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_model = tf.keras.applications.NASNetLarge(\n",
    "                                                    input_shape=(331,331,3),\n",
    "                                                    include_top=False,\n",
    "                                                    weights=\"imagenet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_model(feature_model):\n",
    "    inputs = Input(shape=(None,None,3))\n",
    "    x = feature_model(inputs)\n",
    "    outputs = GlobalAveragePooling2D()(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "feature_model_GAP = build_feature_model(feature_model)\n",
    "feature_model_GAP.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터는 수가 너무많음 따라서 이방법은 train데이터에는 사용 X\n",
    "train_labels = []\n",
    "train_features = []\n",
    "\n",
    "for i in tqdm(range(train_generator.n//batch_size)):\n",
    "    \n",
    "    x, y = train_generator.next()\n",
    "    train_labels.extend(y)\n",
    "    feature = feature_model_GAP.predict(x)\n",
    "    train_features.extend(feature)\n",
    "    \n",
    "train_features = np.array(train_features)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "print(train_features.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user-workspace로 chdir - DenseNet/ HSV\n",
    "# save path\n",
    "current_path = '/home/workspace/user-workspace/'\n",
    "os.chdir(current_path)\n",
    "\n",
    "train_path = './DeepFake/train_features.npy'\n",
    "tlabel_path = './DeepFake/train_labels.npy'\n",
    "\n",
    "valid_path = './DeepFake/valid_features.npy'\n",
    "vlabel_path = './DeepFake/valid_labels.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(train_features,train_path )\n",
    "np.save(train_labels,tlabel_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터는 수가 너무많음 따라서 이방법은 train데이터에는 사용 X\n",
    "valid_labels = []\n",
    "valid_features = []\n",
    "\n",
    "for i in tqdm(range(valid_generator.n//batch_size)):\n",
    "    \n",
    "    x, y = valid_generator.next()\n",
    "    print(x.shape)\n",
    "    valid_labels.extend(y)\n",
    "    feature = feature_model_GAP.predict(x)\n",
    "    valid_features.extend(feature)\n",
    "    \n",
    "valid_features = np.array(valid_features)\n",
    "valid_labels = np.array(valid_labels)\n",
    "\n",
    "print(valid_features.shape)\n",
    "print(valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(valid_features,valid_path )\n",
    "np.save(valid_labels,vlabel_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_outputs=1):\n",
    "    \n",
    "    inputs = Input(shape=feature_model_GAP.output.shape[1:])\n",
    "\n",
    "    x = Dense(1024, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    \n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inputs,\n",
    "                 outputs = outputs)\n",
    "    \n",
    "    model.compile(optimizer=Adam(lr=4e-4),\n",
    "                  loss= 'binary_crossentropy',\n",
    "                  metrics='accuracy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구축\n",
    "model = build_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path=\"./DeepFake/best_model.h5\"\n",
    "\n",
    "cp = ModelCheckpoint(weight_path, monitor='val_accuracy', verbose=1,\n",
    "                        save_best_only=True, save_weights_only=True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, \n",
    "                                   patience=3, \n",
    "                                   min_lr=1e-6)\n",
    "es = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=10)\n",
    "callbacks_list = [cp, es, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs = 100\n",
    "\n",
    "history = model.fit(x = train_features,\n",
    "                    y = train_labels,\n",
    "                    validation_data = (valid_features, valid_labels),\n",
    "                    epochs = initial_epochs,\n",
    "                    steps_per_epoch = train_generator.n//batch_size,\n",
    "                    validation_steps = valid_generator.n//batch_size,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks = callbacks_list\n",
    "                    )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 곡선\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = build_model()\n",
    "\n",
    "# 가중치 로드\n",
    "model2.load_weights(weight_path)\n",
    "\n",
    "# 모델 평가\n",
    "# model2.evaluate(valid_features,valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/workspace/data/.train/.task146/data/public/'\n",
    "os.chdir(data_path)\n",
    "os.getcwd()\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_df = pd.read_csv('open_public_label.txt', header=None)\n",
    "public_df.columns = ['filename']\n",
    "public_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_df.filename = '/home/workspace/data/.train/.task146/data/public/' + public_df.filename\n",
    "public_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prediction\n",
    "# 0 --> 0.5 이하/ 1 --> 0.5 이상\n",
    "public_labels = []\n",
    "unique_Y = [0, 1]\n",
    "model = model2\n",
    "image_size = (331,331)\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(public_df))):\n",
    "    image_path = public_df.filename[i]\n",
    "\n",
    "    # 이미지 불러오기 및 이미지 크기 조정\n",
    "    img = keras.preprocessing.image.load_img(image_path, target_size=image_size)\n",
    "    # 이미지를 array로 변경\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "    # 각 픽셀값을 0과 1사이의 값으로 조정\n",
    "    img = img / 255.0\n",
    "    # 모델의 인풋 타입에 맞게 차원을 하나 늘림\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    feature = feature_model_GAP.predict(img)\n",
    "    #img를 Inception V3의 특징 추출기 feature_model을 활용하여 특징 벡터를 생성. \n",
    "    prediction = model.predict(feature)[0]\n",
    "\n",
    "    label = int(np.round(prediction))\n",
    "    \n",
    "    public_labels.append(label)\n",
    "public_labels = np.array(public_labels)\n",
    "np.save('/home/workspace/user-workspace/DeepFake/public_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_labels = np.load('/home/workspace/user-workspace/DeepFake/public_labels.npy')\n",
    "public_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_df['label'] = public_labels\n",
    "public_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_prediction_path ='/home/workspace/user-workspace/prediction/public_prediction.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_df.to_csv(public_prediction_path, index=False, header=None, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/workspace/data/.train/.task146/data/private/'\n",
    "os.chdir(data_path)\n",
    "os.getcwd()\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_df = pd.read_csv('open_private_label.txt', header=None)\n",
    "private_df.columns = ['filename']\n",
    "private_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_df.filename = '/home/workspace/data/.train/.task146/data/private/' + private_df.filename\n",
    "private_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(image_path,image_size):\n",
    "    # 이미지 불러오기 및 이미지 크기 조정\n",
    "    img = keras.preprocessing.image.load_img(image_path, target_size=image_size)\n",
    "    # 이미지를 array로 변경\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "    # 각 픽셀값을 0과 1사이의 값으로 조정\n",
    "    img = img / 255.0\n",
    "    # 모델의 인풋 타입에 맞게 차원을 하나 늘림\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prediction\n",
    "# 0 --> 0.5 이하/ 1 --> 0.5 이상\n",
    "private_labels = []\n",
    "unique_Y = [0, 1]\n",
    "model = model2\n",
    "image_size = (331,331)\n",
    "batch_size = 32\n",
    "\n",
    "for i in tqdm(range(0,len(private_df), batch_size)):\n",
    "    images = []\n",
    "    \n",
    "    for j in range(batch_size):\n",
    "        image_path = private_df.filename[i+j]\n",
    "        img = pre_processing(image_path,image_size)\n",
    "        images.extend(img)\n",
    "         \n",
    "    images = np.array(images) # shape = (32, 331, 331, 3)\n",
    "    feature = feature_model_GAP.predict(images) # = (32, 4032)\n",
    "    #img를 Inception V3의 특징 추출기 feature_model을 활용하여 특징 벡터를 생성. \n",
    "    prediction = model.predict(feature)\n",
    "#     print(prediction.shape)\n",
    "#     label = int(np.round(prediction))\n",
    "    \n",
    "    private_labels.extend(prediction)\n",
    "#     print(len(private_labels))\n",
    "    \n",
    "private_labels = np.array(private_labels)\n",
    "np.save('/home/workspace/user-workspace/DeepFake/private_labels.npy',private_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_df['label'] = private_labels\n",
    "private_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# private_df.label.round().astype('int')\n",
    "private_df.label = private_df.label.round().astype('int')\n",
    "private_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_prediction_path ='/home/workspace/user-workspace/prediction/private_prediction.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_df.to_csv(private_prediction_path, index=False, header=None, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
